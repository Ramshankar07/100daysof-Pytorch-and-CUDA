{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading scikit_learn-1.5.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.0-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.9/11.0 MB 28.4 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 2.3/11.0 MB 29.0 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 4.0/11.0 MB 36.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.0/11.0 MB 38.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.3/11.0 MB 44.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.1/11.0 MB 43.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 40.9 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 301.8/301.8 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading scipy-1.14.0-cp311-cp311-win_amd64.whl (44.7 MB)\n",
      "   ---------------------------------------- 0.0/44.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.2/44.7 MB 72.0 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 5.3/44.7 MB 68.0 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 8.3/44.7 MB 75.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 10.6/44.7 MB 72.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 13.3/44.7 MB 72.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 15.7/44.7 MB 65.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 20.0/44.7 MB 73.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 22.6/44.7 MB 72.6 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 26.7/44.7 MB 93.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 30.2/44.7 MB 93.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 34.4/44.7 MB 93.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 38.2/44.7 MB 93.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.9/44.7 MB 93.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.2/44.7 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.7/44.7 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.7/44.7 MB 81.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.7/44.7 MB 50.3 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 scipy-1.14.0 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (0.18.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.13.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from matplotlib) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pikal\\anaconda3\\envs\\ag\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision\n",
    "%pip install matplotlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.01\n",
    "BATCH_SIZE = 32\n",
    "EPOCH = 10\n",
    "DOWNLOAD_MNIST = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist/',\n",
    "    train=True,                                     # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,                        # download it if you don't have it\n",
    ")\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist/',\n",
    "    train=False,                                     # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,                        # download it if you don't have it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pikal\\anaconda3\\envs\\ag\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:76: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "c:\\Users\\pikal\\anaconda3\\envs\\ag\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:66: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_data.train_data.size())                 # (60000, 28, 28)\n",
    "print(train_data.train_labels.size())               # (60000)\n",
    "plt.imshow(train_data.train_data[0].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_data.train_labels[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = Data.DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=16,            # n_filters\n",
    "                kernel_size=5,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=2,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (16, 28, 28)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(kernel_size=2),    # choose max value in 2x2 area, output shape (16, 14, 14)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # input shape (1, 28, 28)\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),     # output shape (32, 14, 14)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool2d(2),                # output shape (32, 7, 7)\n",
    "        )\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)           # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        output = self.out(x)\n",
    "        return output, x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = Variable(next(iter(test_loader))[0])\n",
    "test_y = next(iter(test_loader))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 0.1138 | test accuracy: 1.00\n",
      "Epoch:  0 | train loss: 0.1868 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.1177 | test accuracy: 1.00\n",
      "Epoch:  0 | train loss: 0.0130 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.0529 | test accuracy: 0.94\n",
      "Epoch:  0 | train loss: 0.2777 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.0576 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.1333 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.2550 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.0077 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.0731 | test accuracy: 1.00\n",
      "Epoch:  0 | train loss: 0.1161 | test accuracy: 1.00\n",
      "Epoch:  0 | train loss: 0.1150 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.1410 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.0039 | test accuracy: 1.00\n",
      "Epoch:  0 | train loss: 0.1274 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.2271 | test accuracy: 0.97\n",
      "Epoch:  0 | train loss: 0.0417 | test accuracy: 1.00\n",
      "Epoch:  0 | train loss: 0.0121 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0600 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0005 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.0001 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.1291 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0006 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.0463 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.0280 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0002 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.1430 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0034 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.0235 | test accuracy: 0.91\n",
      "Epoch:  1 | train loss: 0.0423 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.0442 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.0198 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.0006 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.0659 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.0031 | test accuracy: 0.94\n",
      "Epoch:  1 | train loss: 0.0649 | test accuracy: 0.97\n",
      "Epoch:  1 | train loss: 0.0195 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.1366 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.0359 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.1109 | test accuracy: 0.94\n",
      "Epoch:  2 | train loss: 0.2802 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.0004 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.2096 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.0009 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.2293 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.0036 | test accuracy: 0.94\n",
      "Epoch:  2 | train loss: 0.0011 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.0061 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.1927 | test accuracy: 0.94\n",
      "Epoch:  2 | train loss: 0.1522 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.0018 | test accuracy: 0.94\n",
      "Epoch:  2 | train loss: 0.0660 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.0051 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.0539 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.0015 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.1485 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0015 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.1413 | test accuracy: 0.94\n",
      "Epoch:  3 | train loss: 0.0022 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0034 | test accuracy: 0.94\n",
      "Epoch:  3 | train loss: 0.0103 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0444 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0827 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.1436 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0006 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0327 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0341 | test accuracy: 1.00\n",
      "Epoch:  3 | train loss: 0.0017 | test accuracy: 1.00\n",
      "Epoch:  3 | train loss: 0.0145 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0829 | test accuracy: 1.00\n",
      "Epoch:  3 | train loss: 0.0003 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0007 | test accuracy: 1.00\n",
      "Epoch:  3 | train loss: 0.0058 | test accuracy: 0.94\n",
      "Epoch:  3 | train loss: 0.2852 | test accuracy: 0.97\n",
      "Epoch:  3 | train loss: 0.0017 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.0110 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.0028 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0936 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0490 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.0897 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.0026 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.0452 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.1840 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.0076 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0411 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.1197 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.0491 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.0617 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.0554 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0017 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.0062 | test accuracy: 0.97\n",
      "Epoch:  4 | train loss: 0.1749 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.1498 | test accuracy: 1.00\n",
      "Epoch:  4 | train loss: 0.2016 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0236 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0175 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0187 | test accuracy: 1.00\n",
      "Epoch:  5 | train loss: 0.0001 | test accuracy: 1.00\n",
      "Epoch:  5 | train loss: 0.2279 | test accuracy: 1.00\n",
      "Epoch:  5 | train loss: 0.1077 | test accuracy: 1.00\n",
      "Epoch:  5 | train loss: 0.0041 | test accuracy: 1.00\n",
      "Epoch:  5 | train loss: 0.1505 | test accuracy: 0.94\n",
      "Epoch:  5 | train loss: 0.0016 | test accuracy: 1.00\n",
      "Epoch:  5 | train loss: 0.0910 | test accuracy: 1.00\n",
      "Epoch:  5 | train loss: 0.0065 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0236 | test accuracy: 1.00\n",
      "Epoch:  5 | train loss: 0.0388 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.1931 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0256 | test accuracy: 1.00\n",
      "Epoch:  5 | train loss: 0.0072 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0160 | test accuracy: 1.00\n",
      "Epoch:  5 | train loss: 0.0421 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0036 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.1279 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.1016 | test accuracy: 1.00\n",
      "Epoch:  6 | train loss: 0.0132 | test accuracy: 0.94\n",
      "Epoch:  6 | train loss: 0.0387 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0063 | test accuracy: 1.00\n",
      "Epoch:  6 | train loss: 0.0111 | test accuracy: 0.94\n",
      "Epoch:  6 | train loss: 0.0090 | test accuracy: 0.94\n",
      "Epoch:  6 | train loss: 0.0094 | test accuracy: 1.00\n",
      "Epoch:  6 | train loss: 0.0287 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0003 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0162 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0490 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0167 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0030 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0009 | test accuracy: 0.94\n",
      "Epoch:  6 | train loss: 0.0221 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0006 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0030 | test accuracy: 0.97\n",
      "Epoch:  6 | train loss: 0.0018 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0120 | test accuracy: 0.94\n",
      "Epoch:  7 | train loss: 0.0873 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0017 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0003 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.6166 | test accuracy: 1.00\n",
      "Epoch:  7 | train loss: 0.1698 | test accuracy: 0.94\n",
      "Epoch:  7 | train loss: 0.1373 | test accuracy: 0.94\n",
      "Epoch:  7 | train loss: 0.0239 | test accuracy: 1.00\n",
      "Epoch:  7 | train loss: 0.0252 | test accuracy: 1.00\n",
      "Epoch:  7 | train loss: 0.0001 | test accuracy: 1.00\n",
      "Epoch:  7 | train loss: 0.0011 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.1189 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0027 | test accuracy: 0.94\n",
      "Epoch:  7 | train loss: 0.0078 | test accuracy: 0.94\n",
      "Epoch:  7 | train loss: 0.0764 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0327 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0004 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0494 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.2063 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.0288 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.0001 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0499 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.1761 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.0321 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.0639 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.0207 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.2785 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.0018 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.0108 | test accuracy: 0.91\n",
      "Epoch:  8 | train loss: 0.0007 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.1842 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.0004 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.2374 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0814 | test accuracy: 1.00\n",
      "Epoch:  8 | train loss: 0.0017 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0466 | test accuracy: 1.00\n",
      "Epoch:  8 | train loss: 0.0000 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.0346 | test accuracy: 1.00\n",
      "Epoch:  9 | train loss: 0.5432 | test accuracy: 1.00\n",
      "Epoch:  9 | train loss: 0.0293 | test accuracy: 1.00\n",
      "Epoch:  9 | train loss: 0.0006 | test accuracy: 1.00\n",
      "Epoch:  9 | train loss: 0.0073 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0012 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0019 | test accuracy: 0.94\n",
      "Epoch:  9 | train loss: 0.0042 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0447 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0011 | test accuracy: 1.00\n",
      "Epoch:  9 | train loss: 0.1834 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.2548 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0002 | test accuracy: 1.00\n",
      "Epoch:  9 | train loss: 0.0281 | test accuracy: 0.94\n",
      "Epoch:  9 | train loss: 0.1467 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0071 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0198 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0255 | test accuracy: 0.94\n",
      "Epoch:  9 | train loss: 0.1604 | test accuracy: 1.00\n",
      "Epoch:  9 | train loss: 0.0423 | test accuracy: 0.94\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x20b1a226f10>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import cm\n",
    "from sklearn.manifold import TSNE\n",
    "def plot_with_labels(lowDWeights, labels):\n",
    "    plt.cla()\n",
    "    X, Y = lowDWeights[:, 0], lowDWeights[:, 1]\n",
    "    for x, y, s in zip(X, Y, labels):\n",
    "        c = cm.rainbow(int(255 * s / 9)); plt.text(x, y, s, backgroundcolor=c, fontsize=9)\n",
    "    plt.xlim(X.min(), X.max()); plt.ylim(Y.min(), Y.max()); plt.title('Visualize last layer'); plt.show(); plt.pause(0.01)\n",
    "\n",
    "plt.ion()\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (x, y) in enumerate(train_loader):   # gives batch data, normalize x when iterate train_loader\n",
    "        b_x = Variable(x)   # batch x\n",
    "        b_y = Variable(y)   # batch y\n",
    "\n",
    "        output = cnn(b_x)[0]               # cnn output\n",
    "        loss = loss_func(output, b_y)   # cross entropy loss\n",
    "        optimizer.zero_grad()           # clear gradients for this training step\n",
    "        loss.backward()                 # backpropagation, compute gradients\n",
    "        optimizer.step()                # apply gradients\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            test_output, last_layer = cnn(test_x)\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "            accuracy = (pred_y == test_y).sum().item() / float(test_y.size(0))\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss.data, '| test accuracy: %.2f' % accuracy)\n",
    "            if HAS_SK:\n",
    "                # Visualization of trained flatten layer (T-SNE)\n",
    "                tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "                plot_only = 500\n",
    "                low_dim_embs = tsne.fit_transform(last_layer.data.numpy()[:plot_only, :])\n",
    "                labels = test_y.numpy()[:plot_only]\n",
    "                plot_with_labels(low_dim_embs, labels)\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 8 9] prediction number\n",
      "[7 2 1 0 4 1 4 9 5 9] real number\n"
     ]
    }
   ],
   "source": [
    "test_output, _ = cnn(test_x[:10])\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "print(pred_y, 'prediction number')\n",
    "print(test_y[:10].numpy(), 'real number')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
